# Academic Presentations

This folder contains selected presentations delivered for various courses. 

---

### The Wisdom of Many in Few: Finding Individuals Who Are as Wise as the Crowd

* **Summary:** This presentation explores methods for identifying talented forecasters in real-time, without requiring event resolutions. It introduces a peer similarity-based intersubjective evaluation method, tested in a unique longitudinal experiment. The research demonstrates that intersubjective accuracy scores, obtained immediately after forecasts, are valid and reliable estimators of forecasting talent. Furthermore, asking forecasters to make meta-predictions about others' beliefs can serve as an incentive-compatible evaluation method. The results suggest that selecting small groups, or even single forecasters, based on intersubjective accuracy can yield forecasts that approximate the accuracy of much larger crowd aggregates.

* **Original Paper (Google Scholar):** [The wisdom of many in few: Finding individuals who are as wise as the crowd](https://scholar.google.com/scholar?q=%22The+wisdom+of+many+in+few%3A+Finding+individuals+who+are+as+wise+as+the+crowd%22)

---

### Going Beyond Simple Sample Size Calculations: A Practitioner's Guide

* **Summary:** This presentation provides a practical guide to advanced sample size calculations, moving beyond basic methods to accommodate complex experimental designs. It compiles methods for continuous and binary outcomes, with and without covariates, for both clustered and non-clustered Randomized Controlled Trials (RCTs), including formulae for panel data and unbalanced designs. Extensions cover optimizing samples under cost constraints, simulating power for complex designs, and adjusting calculations for multiple testing.

* **Original Paper (Google Scholar):** [Going beyond simple sample size calculations: a practitioner's guide](https://scholar.google.com/scholar?q=%22Going+beyond+simple+sample+size+calculations%3A+a+practitioner%27s+guide%22)

---

### Reinforcement Learning in a Prisoner's Dilemma

* **Summary:** This presentation characterizes the outcomes of model-free reinforcement learning algorithms, such as stateless Q-learning, when applied to a Prisoner's Dilemma game. It analyzes player behavior in the limit where experimentation ceases after sufficient exploration. A closed-form relationship between the learning rate and game payoffs is derived, revealing whether players will learn to cooperate or defect. The findings have implications for algorithmic collusion and apply to asymmetric learners with varied experimentation rules.

* **Original Paper (Google Scholar):** [Reinforcement learning in a prisoner's dilemma](https://scholar.google.com/scholar?q=%22Reinforcement+learning+in+a+prisoner%27s+dilemma%22)
